{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT MODEL WITH S-LEARNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "import dowhy\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from dowhy import CausalModel\n",
    "from econml.dml import DML\n",
    "from econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\n",
    "from econml.metalearners import SLearner\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file\n",
    "df = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Review Text'])\n",
    "df = df.dropna(subset=['Age'])\n",
    "df = df.dropna(subset=['Rating'])\n",
    "df = df.dropna(subset=['Recommended IND'])\n",
    "\n",
    "# Convert 'Rating' column to binary: 1 for ratings >= 4, 0 otherwise\n",
    "# This is the treatment variable \n",
    "df['Rating'] = df['Rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random rows where 'Recommended IND' is 1\n",
    "recommended_1 = df[df['Recommended IND'] == 1].sample(n=1000, random_state=42)\n",
    "\n",
    "# Select random rows where 'Recommended IND' is 0, with replacement\n",
    "recommended_0 = df[df['Recommended IND'] == 0].sample(n=1000, replace=True, random_state=42)\n",
    "\n",
    "# Combine the two DataFrames\n",
    "data_file = pd.concat([recommended_1, recommended_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    # Load the DataFrame from the data_file\n",
    "    df = data_file\n",
    "    \n",
    "    # Extract the 'Review Text' and 'Recommended IND' columns\n",
    "    texts = df['Review Text'].tolist()\n",
    "    labels = df['Recommended IND'].tolist()\n",
    "    treatment = df['Rating'].tolist()\n",
    "    confounding = df['Age'].tolist()\n",
    "    # Return the texts and labels\n",
    "    return texts, labels, treatment, confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from the data file\n",
    "# and assigning it to variables 'texts' and 'labels'\n",
    "texts, labels, treatment, confounding = load_data(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASS DEFINITIONS FOR BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the TextClassification dataset for training \n",
    "class TextClassificationDatasetT(Dataset):\n",
    "    def __init__(self, texts, labels, estimated_effects, tokenizer, max_length, padding=True, truncation=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.estimated_effects = estimated_effects\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        effect =  self.estimated_effects[idx] \n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels),\n",
    "            'effects': torch.tensor(effect, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the TextClassification dataset vor validation\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    def __len__(self):\n",
    "            return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            labels = self.labels[idx]\n",
    "            encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "            return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the BERT Classifier \n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            x = self.dropout(pooled_output)\n",
    "            logits = self.fc(x)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE BERT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize \n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 256\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Initialize cross-validation splitter\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store evaluation results\n",
    "accuracy_scores = []\n",
    "classification_reports = []\n",
    "mcc_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING AND EVALUATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        effects = batch['effects'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        weighted_losses = loss * effects\n",
    "        loss = weighted_losses.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "    return preds.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the folds\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(texts, labels)):\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Split data into train and validation sets for this fold\n",
    "    train_texts_fold = [texts[i] for i in train_index]\n",
    "    train_labels_fold = [labels[i] for i in train_index]\n",
    "    train_treatment_fold = [treatment[i] for i in train_index]\n",
    "    train_confounding_fold = [confounding[i] for i in train_index]\n",
    "\n",
    "    val_texts_fold = [texts[i] for i in val_index]\n",
    "    val_labels_fold = [labels[i] for i in val_index]\n",
    "\n",
    "    train_labels_fold = np.array(train_labels_fold)\n",
    "    train_treatment_fold = np.array(train_treatment_fold)\n",
    "    train_confounding_fold = np.array(train_confounding_fold).reshape(-1, 1)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_labels_fold = torch.tensor(train_labels_fold, dtype=torch.long)\n",
    "    train_treatment_fold = torch.tensor(train_treatment_fold)\n",
    "    train_confounding_fold = torch.tensor(train_confounding_fold)\n",
    "\n",
    "    #Initialize the SLearner with RandomForestRegressor as base model\n",
    "    est = SLearner(overall_model=RandomForestRegressor())\n",
    "    \n",
    "    #Fit the SLearner model using the outcome and treatment\n",
    "    est.fit(train_labels_fold, train_treatment_fold)\n",
    "    #Apply the s-learner to the training data \n",
    "    individual_effects = est.effect(train_confounding_fold)\n",
    "    train_slearner_fold = individual_effects    \n",
    "    \n",
    "\n",
    "    # Prepare datasets and dataloaders for this fold\n",
    "    train_dataset_fold = TextClassificationDatasetT(train_texts_fold, train_labels_fold, train_slearner_fold, tokenizer, max_length)\n",
    "    val_dataset_fold = TextClassificationDataset(val_texts_fold, val_labels_fold, tokenizer, max_length)\n",
    "    train_dataloader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader_fold = DataLoader(val_dataset_fold, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize and train model for this fold\n",
    "    model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, no_deprecation_warning=True)\n",
    "    total_steps = len(train_dataloader_fold) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_dataloader_fold, optimizer, scheduler, device)\n",
    "\n",
    "    #Iterate over the validation set and generate predictions\n",
    "    predicted_labels = []\n",
    "    for text in val_texts_fold:\n",
    "        pred = predict(text, model, tokenizer, device)  # Use your predict function here\n",
    "        predicted_labels.append(pred)\n",
    "        \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(val_labels_fold, predicted_labels)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "    # Evaluate model for this fold\n",
    "    accuracy, report = evaluate(model, val_dataloader_fold, device)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    classification_reports.append(report)\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
